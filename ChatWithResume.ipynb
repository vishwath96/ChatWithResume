{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4423b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us import all the required libraries. Ensure that you've run requirements.txt before.\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from pdf2image import convert_from_path\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86631441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define the data path where we have the Resum√©s\n",
    "data_loader = PyPDFDirectoryLoader(\"/data/resume/\")\n",
    "all_resume = data_loader.load()\n",
    "len(all_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we are able to convert PDF to image\n",
    "sample_resume = convert_from_path(\"/data/resume/sampleResume.pdf\", dpi=88)\n",
    "sample_resume[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will build embeddings using the \"hkunlp/instructor-large\" model from HuggingFace\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": DEVICE}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6588f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split the collection of resumes (all_resume) into smaller text chunks \n",
    "split_text = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "all_texts = split_text.split_documents(all_resume)\n",
    "len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185adfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us reates a Chroma database by indexing the embeddings of the input texts\n",
    "# This will facilitate efficient similarity search and retrieval\n",
    "\n",
    "%%time\n",
    "resume_db = Chroma.from_documents(all_texts, embeddings, persist_directory=\"resume_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us initialize GPT-based conversational model from the Hugging Face Transformers\n",
    "# We will be using the Llama-2's chat based model for our experiment\n",
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    revision=\"gptq-4bit-128g-actorder_True\",\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    inject_fused_attention=False,\n",
    "    device=DEVICE,\n",
    "    quantize_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b72dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define a default system prompt\n",
    "system_prompt = \"Use the following pieces of context to answer questions about the candidate's Resume., \"\n",
    "\n",
    "def generate_prompt(prompt, system_prompt):\n",
    "    return f\"\"\"[INST] <<SYS>> \n",
    "    {system_prompt} \n",
    "    <</SYS>> \n",
    "    {prompt} [/INST]\n",
    "    \"\"\".strip()\n",
    "\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea25338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text streamer object using the tokenizer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1740bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us initalize text generation pipeline with (i) max 1024 tokens, (ii) temperature 0\n",
    "# (iii) top-p sampling threshold of 0.95 and repetition penalty of 1.15\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace pipeline with temperature of 0 for deterministic text generation\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create the prompt template pipe\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4333e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now define the QA chain with our resume database as db\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=resume_db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us query with some sample questions\n",
    "sample_questions = [\n",
    "    \"What is the candidate's education?\",\n",
    "    \"Give me the top 3 skills from the Resume.\",\n",
    "    \"What are the key technologies the candidate has worked on?\"\n",
    "    \"Of the lot, pick the best Resume that matches the input job description\"\n",
    "]\n",
    "\n",
    "for question in sample_questions:\n",
    "    qa_chain(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
